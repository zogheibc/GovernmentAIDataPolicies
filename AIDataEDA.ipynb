{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1w2E_XZeS3ahAcH2xSgFnOe51dO3zwf_S","timestamp":1767970804484}],"authorship_tag":"ABX9TyNvqyNm4hIxPxqnsd3od7kU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# !pip -q install --upgrade --force-reinstall --no-cache-dir \\\n","#   \"numpy==1.26.4\" \"scipy==1.12.0\" \"pandas==2.2.2\" \\\n","#   \"spacy==3.7.4\" \"gensim==4.3.3\" \\\n","#   \"langdetect==1.0.9\" \"unidecode==1.3.8\"\n","\n"],"metadata":{"id":"wayt95tnAc1j","executionInfo":{"status":"ok","timestamp":1767987477467,"user_tz":300,"elapsed":9,"user":{"displayName":"ryan spencer","userId":"08004024603382603585"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# # 1) Remove packages that fight with numpy==1.26.4\n","# !pip -q uninstall -y opencv-python opencv-python-headless opencv-contrib-python tsfresh\n","\n","# # 2) Align a few core utilities Colab expects to quiet warnings\n","# !pip -q install --upgrade --force-reinstall \\\n","#   \"requests==2.32.4\" \"jedi==0.19.1\" \"typer==0.12.5\"\n","\n","# # 3) Ensure our NLP stack stays pinned where spaCy is happy\n","# !pip -q install --upgrade --force-reinstall \\\n","#   \"numpy==1.26.4\" \"scipy==1.12.0\" \"pandas==2.2.2\" \\\n","#   \"spacy==3.7.4\" \"gensim==4.3.3\" \"langdetect==1.0.9\" \"unidecode==1.3.8\"\n","\n","# import os\n","# print(\"✅ Cleaned up. Restarting runtime to load consistent binaries…\")\n","# os.kill(os.getpid(), 9)  # Colab-safe restart\n"],"metadata":{"id":"z93xtN5nAY7x","executionInfo":{"status":"ok","timestamp":1767987477475,"user_tz":300,"elapsed":3,"user":{"displayName":"ryan spencer","userId":"08004024603382603585"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["- text_clean_llm (light)\n","- text_clean_tm  (heavy: lowercase, rm punctuation/digits, EN stopwords only, lemmatize, ngrams)"],"metadata":{"id":"-E04rW4W-Bpf"}},{"cell_type":"code","source":["# ============================================================\n","# Preprocess master (EN-only stopwords removal) → CSV only\n","# Source: /content/drive/MyDrive/webscrape_links/master_links_translated_en.csv\n","# Text column: text_en\n","# Output: /content/drive/MyDrive/webscrape_links/master_links_preprocessed.csv\n","# - text_clean_llm (light)\n","# - text_clean_tm  (heavy: lowercase, rm punctuation/digits, EN stopwords only, lemmatize, ngrams)\n","# ============================================================\n","\n","# 0) Install & mount\n","!pip -q install spacy==3.7.4 gensim==4.3.3 unidecode==1.3.8 langdetect==1.0.9\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# 1) Imports & paths\n","from pathlib import Path\n","import csv, re, unicodedata\n","import pandas as pd\n","from unidecode import unidecode\n","from langdetect import detect, DetectorFactory\n","\n","BASE_DIR   = Path(\"/content/drive/MyDrive/webscrape_links\")\n","IN_PATH    = BASE_DIR / \"master_links_translated_en.csv\"   # given\n","OUT_CSV    = BASE_DIR / \"master_links_preprocessed.csv\"\n","\n","# 2) Load data (robust quoting)\n","master = pd.read_csv(IN_PATH, dtype=str, keep_default_na=False, quoting=csv.QUOTE_ALL)\n","assert \"text_en\" in master.columns, \"Expected column 'text_en' not found.\"\n","\n","# 3) Ensure we have a 'lang' column (fr/en from original language if missing)\n","DetectorFactory.seed = 42\n","def detect_lang_en_fr(text: str) -> str:\n","    t = (text or \"\").strip()\n","    if not t:\n","        return \"en\"\n","    try:\n","        return \"fr\" if detect(t) == \"fr\" else \"en\"\n","    except Exception:\n","        return \"en\"\n","\n","if \"lang\" not in master.columns:\n","    # Prefer existing 'language', else detect from original text if present, else from text_en\n","    def derive_lang(row):\n","        lang = (row.get(\"language\",\"\") or \"\").lower()\n","        if lang.startswith(\"fr\"): return \"fr\"\n","        if lang.startswith(\"en\"): return \"en\"\n","        raw = row.get(\"text\",\"\")\n","        return detect_lang_en_fr(raw if raw else row.get(\"text_en\",\"\"))\n","    master[\"lang\"] = master.apply(derive_lang, axis=1)\n","\n","# 4) spaCy EN pipeline (lemmatization & stopwords) — EN stopwords ONLY\n","import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS as EN_STOPWORDS\n","\n","try:\n","    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])\n","except OSError:\n","    import spacy.cli\n","    spacy.cli.download(\"en_core_web_sm\")\n","    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])\n","nlp.add_pipe(\"sentencizer\", first=True)\n","\n","# 5) Core normalizers (applied to both variants)\n","BULLET_CHARS = \"•●◦▪–—‒―·∙\"\n","BULLET_REGEX = re.compile(f\"[{re.escape(BULLET_CHARS)}]\")\n","\n","def unicode_nfkc(text: str) -> str:\n","    return unicodedata.normalize(\"NFKC\", text or \"\")\n","\n","def fix_dehyphenation(text: str) -> str:\n","    # collapse word-\\nword -> wordword\n","    return re.sub(r\"(\\w)-\\s*\\n\\s*(\\w)\", r\"\\1\\2\", text)\n","\n","def normalize_bullets(text: str) -> str:\n","    return BULLET_REGEX.sub(\"-\", text)\n","\n","def collapse_whitespace(text: str) -> str:\n","    # keep paragraph breaks: >1 blank lines -> exactly two \\n\n","    t = re.sub(r\"\\r\\n?\", \"\\n\", text)\n","    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n","    t = re.sub(r\"[ \\t]{2,}\", \" \", t)\n","    return t.strip()\n","\n","def basic_clean(text: str) -> str:\n","    t = unicode_nfkc(text)\n","    t = fix_dehyphenation(t)\n","    t = normalize_bullets(t)\n","    t = collapse_whitespace(t)\n","    return t\n","\n","# 6) Light cleaning for LLM (preserve case/punct/numbers)\n","def build_text_clean_llm(text: str) -> str:\n","    return basic_clean(text)\n","\n","# 7) Heavy cleaning for topic modeling (EN stopwords only)\n","PUNCT_REGEX = re.compile(r\"[^\\w\\s]\")\n","DIGIT_REGEX = re.compile(r\"\\d\")\n","\n","# Optional domain stopwords (EN only). You can add items later:\n","DOMAIN_STOPWORDS_EN = set([\n","    # e.g., \"canada\",\"government\",\"policy\",\"directive\",\"annex\",\"appendix\",\"montreal\"\n","])\n","\n","def tokenize_lemma_en(doc):\n","    out = []\n","    for tok in doc:\n","        if tok.is_space or tok.is_punct:\n","            continue\n","        lemma = tok.lemma_.strip().lower()\n","        if not lemma:\n","            continue\n","        # drop digits & punctuation\n","        if DIGIT_REGEX.search(lemma):\n","            continue\n","        lemma = PUNCT_REGEX.sub(\"\", lemma)\n","        if not lemma or len(lemma) < 3:\n","            continue\n","        # EN stopwords ONLY\n","        if lemma in EN_STOPWORDS or lemma in DOMAIN_STOPWORDS_EN:\n","            continue\n","        out.append(lemma)\n","    return out\n","\n","# 8) Build both variants\n","source_texts = master[\"text_en\"].fillna(\"\")\n","\n","# Light\n","master[\"text_clean_llm\"] = [build_text_clean_llm(t) for t in source_texts]\n","master[\"llm_char_count\"] = master[\"text_clean_llm\"].map(lambda t: str(len(t)))\n","master[\"llm_word_count\"] = master[\"text_clean_llm\"].map(lambda t: str(len(t.split())))\n","\n","# Heavy – process with spaCy in batches for speed\n","base_cleaned = [basic_clean(t).lower() for t in source_texts]  # lower for stability\n","tokens_tm = []\n","for doc in nlp.pipe(base_cleaned, batch_size=16, n_process=1):\n","    tokens_tm.append(tokenize_lemma_en(doc))\n","\n","# 9) Learn bigrams/trigrams across corpus and apply\n","from gensim.models.phrases import Phrases, Phraser\n","phrases = Phrases(tokens_tm, min_count=2, threshold=10.0, delimiter=b\"_\")\n","bigram  = Phraser(phrases)\n","trigram = Phraser(Phrases(bigram[tokens_tm], min_count=2, threshold=10.0, delimiter=b\"_\"))\n","\n","tokens_tm_ngrams = [list(trigram[bigram[toks]]) for toks in tokens_tm]\n","\n","# Serialize heavy outputs as space-joined strings (CSV-friendly)\n","master[\"text_clean_tm\"]        = [\" \".join(toks) for toks in tokens_tm]\n","master[\"text_clean_tm_ngrams\"] = [\" \".join(toks) for toks in tokens_tm_ngrams]\n","master[\"tm_char_count\"]        = master[\"text_clean_tm\"].map(lambda t: str(len(t)))\n","master[\"tm_word_count\"]        = master[\"text_clean_tm\"].map(lambda t: str(len(t.split())))\n","master[\"tm_ng_char_count\"]     = master[\"text_clean_tm_ngrams\"].map(lambda t: str(len(t)))\n","master[\"tm_ng_word_count\"]     = master[\"text_clean_tm_ngrams\"].map(lambda t: str(len(t.split())))\n","\n","# 10) Save CSV (fully quoted to preserve newlines)\n","master.to_csv(OUT_CSV, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","print(f\"[OK] Wrote: {OUT_CSV}\")\n","\n","print(\"\\nPreview of added columns:\\n\", master[[\n","    \"doc_id\",\"lang\",\"llm_word_count\",\"tm_word_count\",\"tm_ng_word_count\"\n","]].head().to_string(index=False))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":862},"id":"hcHXMaHg2FJf","outputId":"ddd7b117-e2a8-44d1-9f15-9140551f5757","executionInfo":{"status":"error","timestamp":1767987662901,"user_tz":300,"elapsed":185375,"user":{"displayName":"ryan spencer","userId":"08004024603382603585"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m798.7/981.5 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.1/183.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n","pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","gradio 5.50.0 requires typer<1.0,>=0.12, but you have typer 0.9.4 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","access 1.1.10.post3 requires scipy>=1.14.1, but you have scipy 1.13.1 which is incompatible.\n","shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n","jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n","jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]},{"output_type":"error","ename":"ValueError","evalue":"mount failed","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3662905522.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 1) Imports & paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed"]}]},{"cell_type":"markdown","source":["- Summary Statistics"],"metadata":{"id":"zGw65OMcC6-p"}},{"cell_type":"markdown","source":["top 20 word\n","top 20 ngrams"],"metadata":{"id":"i2djJNLFEJo1"}},{"cell_type":"code","source":["# ============================================================\n","# Top-20 terms per document for BOTH TM streams:\n","#   • text_clean_tm          → column: top20_terms_tm\n","#   • text_clean_tm_ngrams   → column: top20_terms_tm_ngrams\n","#\n","# Reads:  /content/drive/MyDrive/webscrape_links/master_links_preprocessed.csv\n","# Writes: same CSV updated with two new columns\n","# Plots:  PNGs to:\n","#   /content/drive/MyDrive/webscrape_links/viz/top20_terms/tm/\n","#   /content/drive/MyDrive/webscrape_links/viz/top20_terms/tm_ngrams/\n","# ============================================================\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","from pathlib import Path\n","import csv, re\n","from collections import Counter\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# -------------------------\n","# Paths\n","# -------------------------\n","BASE_DIR    = Path(\"/content/drive/MyDrive/webscrape_links\")\n","MASTER_PATH = BASE_DIR / \"master_links_preprocessed.csv\"\n","VIZ_TM      = BASE_DIR / \"viz\" / \"top20_terms\" / \"tm\"\n","VIZ_TMN     = BASE_DIR / \"viz\" / \"top20_terms\" / \"tm_ngrams\"\n","VIZ_TM.mkdir(parents=True, exist_ok=True)\n","VIZ_TMN.mkdir(parents=True, exist_ok=True)\n","\n","# -------------------------\n","# Load master (robust quoting)\n","# -------------------------\n","df = pd.read_csv(MASTER_PATH, dtype=str, keep_default_na=False, quoting=csv.QUOTE_ALL)\n","assert \"text_clean_tm\" in df.columns, \"Missing column 'text_clean_tm' in master.\"\n","assert \"text_clean_tm_ngrams\" in df.columns, \"Missing column 'text_clean_tm_ngrams' in master.\"\n","\n","# -------------------------\n","# Helpers\n","# -------------------------\n","def topn(space_joined: str, n: int = 20):\n","    \"\"\"Return (terms, counts) top-N from a space-joined token string.\"\"\"\n","    if not space_joined:\n","        return [], []\n","    toks = [t for t in space_joined.split() if t]\n","    if not toks:\n","        return [], []\n","    cnt = Counter(toks).most_common(n)\n","    terms = [t for t, c in cnt]\n","    counts = [c for t, c in cnt]\n","    return terms, counts\n","\n","def safe_fname(s: str, fallback: str, maxlen: int = 60):\n","    s = (s or \"\").strip() or fallback\n","    s = re.sub(r\"[^A-Za-z0-9_.-]+\", \"_\", s)\n","    return s[:maxlen]\n","\n","# -------------------------\n","# Compute & store top20 for both streams + plots\n","# -------------------------\n","top20_tm_col  = []\n","top20_tmn_col = []\n","tm_plots      = []\n","tmn_plots     = []\n","\n","for idx, row in df.iterrows():\n","    doc_id = row.get(\"doc_id\") or f\"row{idx}\"\n","    fname  = safe_fname(doc_id, f\"row{idx}\")\n","\n","    # --- Plain TM ---\n","    t_tm, c_tm = topn(row.get(\"text_clean_tm\", \"\"), n=20)\n","    top20_tm_col.append(\"; \".join(f\"{t}:{c}\" for t, c in zip(t_tm, c_tm)))\n","\n","    if t_tm:\n","        out_png_tm = VIZ_TM / f\"{idx:02d}_{fname}.png\"\n","        plt.figure(figsize=(12, 6))\n","        plt.bar(range(len(t_tm)), c_tm)\n","        plt.xticks(range(len(t_tm)), t_tm, rotation=45, ha=\"right\")\n","        plt.title(f\"Top 20 terms • {doc_id} • TM (no n-grams)\")\n","        plt.xlabel(\"term\")\n","        plt.ylabel(\"count\")\n","        plt.tight_layout()\n","        plt.savefig(out_png_tm, dpi=150)\n","        plt.close()\n","        tm_plots.append(str(out_png_tm))\n","    else:\n","        tm_plots.append(\"\")\n","\n","    # --- TM + n-grams ---\n","    t_tmn, c_tmn = topn(row.get(\"text_clean_tm_ngrams\", \"\"), n=20)\n","    top20_tmn_col.append(\"; \".join(f\"{t}:{c}\" for t, c in zip(t_tmn, c_tmn)))\n","\n","    if t_tmn:\n","        out_png_tmn = VIZ_TMN / f\"{idx:02d}_{fname}.png\"\n","        plt.figure(figsize=(12, 6))\n","        plt.bar(range(len(t_tmn)), c_tmn)\n","        plt.xticks(range(len(t_tmn)), t_tmn, rotation=45, ha=\"right\")\n","        plt.title(f\"Top 20 terms • {doc_id} • TM n-grams\")\n","        plt.xlabel(\"term\")\n","        plt.ylabel(\"count\")\n","        plt.tight_layout()\n","        plt.savefig(out_png_tmn, dpi=150)\n","        plt.close()\n","        tmn_plots.append(str(out_png_tmn))\n","    else:\n","        tmn_plots.append(\"\")\n","\n","# -------------------------\n","# Add columns & save master\n","# -------------------------\n","df[\"top20_terms_tm\"]        = top20_tm_col\n","df[\"top20_terms_tm_ngrams\"] = top20_tmn_col\n","\n","df.to_csv(MASTER_PATH, index=False, encoding=\"utf-8\",\n","          quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","\n","print(f\"[OK] Added `top20_terms_tm` and `top20_terms_tm_ngrams` → {MASTER_PATH}\")\n","print(f\"[OK] Wrote {sum(bool(p) for p in tm_plots)} TM plots → {VIZ_TM}\")\n","print(f\"[OK] Wrote {sum(bool(p) for p in tmn_plots)} TM n-gram plots → {VIZ_TMN}\")\n","\n","# -------------------------\n","# Preview the first pair inline (if any)\n","# -------------------------\n","from IPython.display import Image, display\n","\n","first_tm  = next((p for p in tm_plots if p), None)\n","first_tmn = next((p for p in tmn_plots if p), None)\n","\n","if first_tm:\n","    print(\"\\nPreview: TM (no n-grams)\")\n","    display(Image(filename=first_tm))\n","if first_tmn:\n","    print(\"\\nPreview: TM n-grams\")\n","    display(Image(filename=first_tmn))\n","if not first_tm and not first_tmn:\n","    print(\"No plots to preview (no tokens found).\")\n"],"metadata":{"id":"wruluIb2BP_Z","executionInfo":{"status":"aborted","timestamp":1767987663267,"user_tz":300,"elapsed":375,"user":{"displayName":"ryan spencer","userId":"08004024603382603585"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- reads your preprocessed master at /content/drive/MyDrive/webscrape_links/master_links_preprocessed.csv\n","\n","scans text_clean_tm_ngrams token streams\n","\n","collects bigrams and trigrams that include either data or artificial_intelligence\n","\n","bigrams_before: X_data, X_artificial_intelligence\n","\n","bigrams_after: data_X, artificial_intelligence_X\n","\n","trigrams_before: X_Y_data, X_Y_artificial_intelligence\n","\n","trigrams_middle: X_data_Y, X_artificial_intelligence_Y\n","\n","trigrams_after: data_X_Y, artificial_intelligence_X_Y\n","\n","prints the top 20 for each category, and\n","\n","saves a tidy CSV summary ngram_contexts_data_ai.csv with columns: target, context_type, ngram, count"],"metadata":{"id":"BoP-trOYEm5U"}},{"cell_type":"code","source":["# ============================================================\n","# N-gram contexts around 'data' and 'artificial_intelligence'\n","# - Reads:  /content/drive/MyDrive/webscrape_links/master_links_preprocessed.csv\n","# - Uses:   text_clean_tm_ngrams (space-joined tokens with n-grams)\n","# - Outputs:\n","#     * Prints top-20 bigrams & trigrams (before/middle/after) for each target\n","#     * Saves CSV: ngram_contexts_data_ai.csv with all counts\n","# ============================================================\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","from pathlib import Path\n","import csv\n","import pandas as pd\n","from collections import Counter, defaultdict\n","\n","BASE_DIR    = Path(\"/content/drive/MyDrive/webscrape_links\")\n","MASTER_PATH = BASE_DIR / \"master_links_preprocessed.csv\"\n","OUT_PATH    = BASE_DIR / \"ngram_contexts_data_ai.csv\"\n","\n","# ---------- Load ----------\n","df = pd.read_csv(MASTER_PATH, dtype=str, keep_default_na=False, quoting=csv.QUOTE_ALL)\n","assert \"text_clean_tm_ngrams\" in df.columns, \"Missing 'text_clean_tm_ngrams'. Run preprocessing first.\"\n","\n","docs = [t.split() if t else [] for t in df[\"text_clean_tm_ngrams\"].tolist()]\n","\n","# ---------- Helpers ----------\n","def accumulate_contexts_for_target(docs_tokens, target_token, fallback_pair=None):\n","    \"\"\"\n","    Collect bigram/trigram contexts around a target token.\n","    If target_token never appears and fallback_pair=('artificial','intelligence') is provided,\n","    detect adjacency and treat as the target in position i (the 'artificial' index).\n","    Returns a dict of Counters for:\n","      bigram_before, bigram_after, trigram_before, trigram_middle, trigram_after\n","    \"\"\"\n","    C = {\n","        \"bigram_before\": Counter(),\n","        \"bigram_after\": Counter(),\n","        \"trigram_before\": Counter(),  # X_Y_target\n","        \"trigram_middle\": Counter(),  # X_target_Y\n","        \"trigram_after\": Counter(),   # target_X_Y\n","    }\n","    seen_target_any = False\n","\n","    for toks in docs_tokens:\n","        n = len(toks)\n","        i = 0\n","        while i < n:\n","            is_target = (toks[i] == target_token)\n","            used_fallback_here = False\n","\n","            # Fallback: treat 'artificial' + 'intelligence' as target if requested and token absent\n","            if not is_target and fallback_pair and i < n - 1:\n","                if toks[i] == fallback_pair[0] and toks[i+1] == fallback_pair[1]:\n","                    is_target = True\n","                    used_fallback_here = True  # target starts at i, spans i..i+1\n","\n","            if is_target:\n","                seen_target_any = True\n","\n","                # Bigrams\n","                if i > 0:\n","                    C[\"bigram_before\"][f\"{toks[i-1]}_{target_token}\"] += 1\n","                if (i < n - 1) and not used_fallback_here:\n","                    C[\"bigram_after\"][f\"{target_token}_{toks[i+1]}\"] += 1\n","                elif used_fallback_here and (i + 2 < n):\n","                    # Fallback spans two tokens; \"after\" neighbor is i+2\n","                    C[\"bigram_after\"][f\"{target_token}_{toks[i+2]}\"] += 1\n","\n","                # Trigrams\n","                # before: X_Y_target (two tokens before target)\n","                if i > 1:\n","                    C[\"trigram_before\"][f\"{toks[i-2]}_{toks[i-1]}_{target_token}\"] += 1\n","\n","                # middle: X_target_Y (one before, one after)\n","                if i > 0 and not used_fallback_here and (i < n - 1):\n","                    C[\"trigram_middle\"][f\"{toks[i-1]}_{target_token}_{toks[i+1]}\"] += 1\n","                elif used_fallback_here:\n","                    # middle span is (i-1) + target + (i+2)\n","                    if i > 0 and (i + 2 < n):\n","                        C[\"trigram_middle\"][f\"{toks[i-1]}_{target_token}_{toks[i+2]}\"] += 1\n","\n","                # after: target_X_Y (two tokens after target)\n","                if not used_fallback_here and (i + 2 < n):\n","                    C[\"trigram_after\"][f\"{target_token}_{toks[i+1]}_{toks[i+2]}\"] += 1\n","                elif used_fallback_here and (i + 3 < n):\n","                    C[\"trigram_after\"][f\"{target_token}_{toks[i+2]}_{toks[i+3]}\"] += 1\n","\n","                # Advance pointer: if fallback used (two-token target), skip extra token\n","                i += 2 if used_fallback_here else 1\n","            else:\n","                i += 1\n","\n","    return C, seen_target_any\n","\n","def print_top(counter, k=20):\n","    items = counter.most_common(k)\n","    if not items:\n","        print(\"  (none)\")\n","    for term, cnt in items:\n","        print(f\"  {term:50s}  {cnt}\")\n","\n","# ---------- Accumulate contexts ----------\n","data_ctx, _ = accumulate_contexts_for_target(docs, target_token=\"data\", fallback_pair=None)\n","ai_ctx, seen_ai = accumulate_contexts_for_target(\n","    docs,\n","    target_token=\"artificial_intelligence\",\n","    fallback_pair=(\"artificial\", \"intelligence\")  # fallback if bigram wasn't formed\n",")\n","\n","# ---------- Print top-20 for each category ----------\n","def show_ctx(title, ctx):\n","    print(f\"\\n=== {title}: BIGRAMS (before) ===\")\n","    print_top(ctx[\"bigram_before\"], 20)\n","    print(f\"\\n=== {title}: BIGRAMS (after) ===\")\n","    print_top(ctx[\"bigram_after\"], 20)\n","    print(f\"\\n=== {title}: TRIGRAMS (before) ===\")\n","    print_top(ctx[\"trigram_before\"], 20)\n","    print(f\"\\n=== {title}: TRIGRAMS (middle) ===\")\n","    print_top(ctx[\"trigram_middle\"], 20)\n","    print(f\"\\n=== {title}: TRIGRAMS (after) ===\")\n","    print_top(ctx[\"trigram_after\"], 20)\n","\n","show_ctx(\"DATA\", data_ctx)\n","show_ctx(\"ARTIFICIAL_INTELLIGENCE\", ai_ctx)\n","\n","# ---------- Save tidy CSV with ALL counts ----------\n","rows = []\n","for target, ctx in [(\"data\", data_ctx), (\"artificial_intelligence\", ai_ctx)]:\n","    for ctype, counter in ctx.items():\n","        for ngram, cnt in counter.items():\n","            rows.append({\n","                \"target\": target,\n","                \"context_type\": ctype,   # bigram_before / bigram_after / trigram_before / trigram_middle / trigram_after\n","                \"ngram\": ngram,\n","                \"count\": cnt\n","            })\n","\n","pd.DataFrame(rows, columns=[\"target\",\"context_type\",\"ngram\",\"count\"]).sort_values(\n","    [\"target\",\"context_type\",\"count\"], ascending=[True, True, False]\n",").to_csv(OUT_PATH, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","\n","print(f\"\\n[OK] Saved full context counts → {OUT_PATH}\")\n"],"metadata":{"id":"nxOSHuqwEQ3m","executionInfo":{"status":"aborted","timestamp":1767987663272,"user_tz":300,"elapsed":378,"user":{"displayName":"ryan spencer","userId":"08004024603382603585"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Input: /content/drive/MyDrive/webscrape_links/master_links_preprocessed.csv\n","\n","Text source: prefers text_clean_tm_ngrams, falls back to text_clean_tm if needed\n","\n","Outputs (in the same folder):\n","\n","tfidf_cosine_matrix.csv — square matrix (rows/cols = doc_id)\n","\n","tfidf_cosine_pairs.csv — tidy upper-triangle pairs (doc_id_1, doc_id_2, cosine), sorted desc"],"metadata":{"id":"ejS2qKGyGMoK"}},{"cell_type":"code","source":["# ============================================================\n","# TF-IDF + Cosine similarity (pairwise) for master docs\n","# Reads:  /content/drive/MyDrive/webscrape_links/master_links_preprocessed.csv\n","# Uses:   text_clean_tm_ngrams (preferred) else text_clean_tm\n","# Writes: tfidf_cosine_matrix.csv (NxN) and tfidf_cosine_pairs.csv (long form)\n","# ============================================================\n","\n","\n","\n","from pathlib import Path\n","import csv\n","import numpy as np\n","import pandas as pd\n","\n","# 0) Paths\n","BASE_DIR   = Path(\"/content/drive/MyDrive/webscrape_links\")\n","IN_PATH    = BASE_DIR / \"master_links_preprocessed.csv\"\n","MATRIX_CSV = BASE_DIR / \"tfidf_cosine_matrix.csv\"\n","PAIRS_CSV  = BASE_DIR / \"tfidf_cosine_pairs.csv\"\n","\n","# 1) Load master\n","df = pd.read_csv(IN_PATH, dtype=str, keep_default_na=False, quoting=csv.QUOTE_ALL)\n","\n","# 2) Pick source text column\n","SRC_COL = None\n","if \"text_clean_tm_ngrams\" in df.columns and df[\"text_clean_tm_ngrams\"].str.strip().any():\n","    SRC_COL = \"text_clean_tm_ngrams\"\n","elif \"text_clean_tm\" in df.columns and df[\"text_clean_tm\"].str.strip().any():\n","    SRC_COL = \"text_clean_tm\"\n","else:\n","    raise RuntimeError(\"No usable text column found. Expected 'text_clean_tm_ngrams' or 'text_clean_tm'.\")\n","\n","print(f\"[INFO] Using column for TF-IDF: {SRC_COL}\")\n","\n","texts  = df[SRC_COL].fillna(\"\").astype(str).tolist()\n","docids = df[\"doc_id\"].fillna(\"\").astype(str).tolist()\n","\n","# 3) Build TF-IDF (prefer existing sklearn; install if missing)\n","try:\n","    from sklearn.feature_extraction.text import TfidfVectorizer\n","    from sklearn.metrics.pairwise import cosine_similarity\n","except Exception:\n","    !pip -q install scikit-learn==1.5.1\n","    from sklearn.feature_extraction.text import TfidfVectorizer\n","    from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Use a token pattern that keeps underscores (already space-separated tokens)\n","vectorizer = TfidfVectorizer(\n","    lowercase=False,              # already lowercased upstream\n","    token_pattern=r\"(?u)\\b\\w[\\w_]+\\b\",  # keep tokens with underscores\n","    ngram_range=(1,1),            # we already have phrases joined with \"_\"\n","    min_df=1                      # keep all terms\n",")\n","\n","# 4) Fit & transform\n","X = vectorizer.fit_transform(texts)           # shape: (N_docs, N_terms)\n","if X.shape[1] == 0:\n","    raise RuntimeError(\"TF-IDF vocabulary is empty. Check that your text column has tokens.\")\n","\n","# 5) Cosine similarity (dense NxN)\n","sim = cosine_similarity(X)                    # numpy array (N x N), diagonal = 1.0\n","\n","# 6) Save matrix CSV (NxN with doc_id headers)\n","matrix_df = pd.DataFrame(sim, index=docids, columns=docids)\n","matrix_df.to_csv(MATRIX_CSV, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","print(f\"[OK] Wrote cosine matrix → {MATRIX_CSV}\")\n","\n","# 7) Save pairs CSV (upper triangle, excluding diagonal)\n","rows = []\n","N = len(docids)\n","for i in range(N):\n","    for j in range(i+1, N):\n","        rows.append({\"doc_id_1\": docids[i], \"doc_id_2\": docids[j], \"cosine\": float(sim[i, j])})\n","\n","pairs_df = pd.DataFrame(rows).sort_values(\"cosine\", ascending=False)\n","pairs_df.to_csv(PAIRS_CSV, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","print(f\"[OK] Wrote cosine pairs → {PAIRS_CSV}\")\n","\n","# 8) Quick peek\n","print(\"\\nTop 10 most similar pairs:\")\n","print(pairs_df.head(10).to_string(index=False))\n"],"metadata":{"id":"wjS418hSFSqU","executionInfo":{"status":"aborted","timestamp":1767987663275,"user_tz":300,"elapsed":380,"user":{"displayName":"ryan spencer","userId":"08004024603382603585"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["TF-IDF + cosine similarity using text_clean_tm (unigrams only), and saves results to their own CSVs:\n","\n","tfidf_cosine_matrix_tm.csv — square matrix (rows/cols = doc_id)\n","\n","tfidf_cosine_pairs_tm.csv — tidy upper-triangle pairs (doc_id_1, doc_id_2, cosine), sorted desc"],"metadata":{"id":"JKiuZyNiGnAx"}},{"cell_type":"code","source":["# ============================================================\n","# TF-IDF + Cosine similarity using text_clean_tm (unigrams)\n","# Reads:  /content/drive/MyDrive/webscrape_links/master_links_preprocessed.csv\n","# Uses:   text_clean_tm\n","# Writes: tfidf_cosine_matrix_tm.csv (NxN) and tfidf_cosine_pairs_tm.csv (long form)\n","# ============================================================\n","\n","\n","from pathlib import Path\n","import csv\n","import numpy as np\n","import pandas as pd\n","\n","# 0) Paths\n","BASE_DIR   = Path(\"/content/drive/MyDrive/webscrape_links\")\n","IN_PATH    = BASE_DIR / \"master_links_preprocessed.csv\"\n","MATRIX_CSV = BASE_DIR / \"tfidf_cosine_matrix_tm.csv\"\n","PAIRS_CSV  = BASE_DIR / \"tfidf_cosine_pairs_tm.csv\"\n","\n","# 1) Load master\n","df = pd.read_csv(IN_PATH, dtype=str, keep_default_na=False, quoting=csv.QUOTE_ALL)\n","assert \"doc_id\" in df.columns, \"Missing 'doc_id' in master.\"\n","assert \"text_clean_tm\" in df.columns, \"Missing 'text_clean_tm' in master. Run preprocessing first.\"\n","\n","texts  = df[\"text_clean_tm\"].fillna(\"\").astype(str).tolist()\n","docids = df[\"doc_id\"].fillna(\"\").astype(str).tolist()\n","\n","# 2) Build TF-IDF (install sklearn if needed)\n","try:\n","    from sklearn.feature_extraction.text import TfidfVectorizer\n","    from sklearn.metrics.pairwise import cosine_similarity\n","except Exception:\n","    !pip -q install scikit-learn==1.5.1\n","    from sklearn.feature_extraction.text import TfidfVectorizer\n","    from sklearn.metrics.pairwise import cosine_similarity\n","\n","# token pattern keeps word characters; tokens are already space-joined\n","vectorizer = TfidfVectorizer(\n","    lowercase=False,                  # already lowercased upstream\n","    token_pattern=r\"(?u)\\b\\w+\\b\",     # unigrams; underscores uncommon in tm stream\n","    ngram_range=(1,1),\n","    min_df=1\n",")\n","\n","# 3) Fit & transform\n","X = vectorizer.fit_transform(texts)           # (N_docs, N_terms)\n","if X.shape[1] == 0:\n","    raise RuntimeError(\"TF-IDF vocabulary is empty. Check that 'text_clean_tm' has tokens.\")\n","\n","# 4) Cosine similarity (dense NxN); guard against empty rows\n","sim = cosine_similarity(X)                    # numpy array (N x N)\n","sim = np.nan_to_num(sim, nan=0.0, posinf=0.0, neginf=0.0)\n","np.fill_diagonal(sim, 1.0)\n","\n","# 5) Save matrix CSV (NxN with doc_id headers)\n","matrix_df = pd.DataFrame(sim, index=docids, columns=docids)\n","matrix_df.to_csv(MATRIX_CSV, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","print(f\"[OK] Wrote cosine matrix (tm) → {MATRIX_CSV}\")\n","\n","# 6) Save pairs CSV (upper triangle, excluding diagonal)\n","rows = []\n","N = len(docids)\n","for i in range(N):\n","    for j in range(i+1, N):\n","        rows.append({\"doc_id_1\": docids[i], \"doc_id_2\": docids[j], \"cosine\": float(sim[i, j])})\n","\n","pairs_df = pd.DataFrame(rows).sort_values(\"cosine\", ascending=False)\n","pairs_df.to_csv(PAIRS_CSV, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","print(f\"[OK] Wrote cosine pairs (tm) → {PAIRS_CSV}\")\n","\n","# 7) Quick peek\n","print(\"\\nTop 10 most similar pairs (tm):\")\n","print(pairs_df.head(10).to_string(index=False))\n"],"metadata":{"id":"C_4TAYaiGSbd","executionInfo":{"status":"aborted","timestamp":1767987663280,"user_tz":300,"elapsed":383,"user":{"displayName":"ryan spencer","userId":"08004024603382603585"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["BM25 similarities between all documents using your text_clean_tm (unigrams). It produces:\n","\n","bm25_matrix_tm.csv — symmetric BM25 similarity matrix (rows/cols = doc_id)\n","\n","bm25_pairs_tm.csv — tidy list of pairs (doc_id_1, doc_id_2, bm25_raw, bm25_norm), sorted by similarity (desc)"],"metadata":{"id":"o15LnwQ0HL5H"}},{"cell_type":"code","source":["# ============================================================\n","# BM25 (symmetric) pairwise similarities using text_clean_tm\n","# Reads:  /content/drive/MyDrive/webscrape_links/master_links_preprocessed.csv\n","# Uses:   text_clean_tm (space-joined tokens)\n","# Writes: bm25_matrix_tm.csv (NxN) and bm25_pairs_tm.csv (long form)\n","# ============================================================\n","\n","\n","\n","from pathlib import Path\n","import csv\n","import numpy as np\n","import pandas as pd\n","\n","# 0) Paths\n","BASE_DIR   = Path(\"/content/drive/MyDrive/webscrape_links\")\n","IN_PATH    = BASE_DIR / \"master_links_preprocessed.csv\"\n","MATRIX_CSV = BASE_DIR / \"bm25_matrix_tm.csv\"\n","PAIRS_CSV  = BASE_DIR / \"bm25_pairs_tm.csv\"\n","\n","# 1) Load master\n","df = pd.read_csv(IN_PATH, dtype=str, keep_default_na=False, quoting=csv.QUOTE_ALL)\n","assert \"doc_id\" in df.columns, \"Missing 'doc_id' in master.\"\n","assert \"text_clean_tm\" in df.columns, \"Missing 'text_clean_tm' in master. Run preprocessing first.\"\n","\n","# Token lists from space-joined stream\n","docs_tokens = [(t or \"\").split() for t in df[\"text_clean_tm\"].tolist()]\n","docids      = df[\"doc_id\"].fillna(\"\").astype(str).tolist()\n","\n","# 2) BM25 (install rank_bm25 if needed)\n","try:\n","    from rank_bm25 import BM25Okapi\n","except Exception:\n","    !pip -q install rank-bm25==0.2.2\n","    from rank_bm25 import BM25Okapi\n","\n","# Build BM25 index on the corpus (unigram tokens)\n","bm25 = BM25Okapi(docs_tokens)  # default k1=1.5, b=0.75 (good general defaults)\n","\n","N = len(docs_tokens)\n","# Asymmetric matrix: score(query_i, doc_j)\n","asym = np.zeros((N, N), dtype=float)\n","for i, q in enumerate(docs_tokens):\n","    scores = bm25.get_scores(q)  # scores for all docs vs query q\n","    asym[i, :] = scores\n","\n","# 3) Make it symmetric for \"similarity\" interpretation:\n","#    S(i,j) = (BM25(i->j) + BM25(j->i)) / 2\n","sym = (asym + asym.T) / 2.0\n","\n","# Diagonal: make self-similarity the max of row (or keep computed value). We'll set to row max for visibility.\n","for i in range(N):\n","    sym[i, i] = max(sym[i, :]) if N else 0.0\n","\n","# 4) Save matrix CSV (NxN with doc_id headers)\n","matrix_df = pd.DataFrame(sym, index=docids, columns=docids)\n","matrix_df.to_csv(MATRIX_CSV, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","print(f\"[OK] Wrote BM25 symmetric matrix (tm) → {MATRIX_CSV}\")\n","\n","# 5) Save pairs CSV (upper triangle, excluding diagonal), with normalized score\n","rows = []\n","# Compute min-max across off-diagonal for normalization\n","off_diag = [sym[i, j] for i in range(N) for j in range(N) if j > i]\n","min_s = float(min(off_diag)) if off_diag else 0.0\n","max_s = float(max(off_diag)) if off_diag else 1.0\n","rng   = (max_s - min_s) if (max_s > min_s) else 1.0\n","\n","for i in range(N):\n","    for j in range(i+1, N):\n","        raw = float(sym[i, j])\n","        norm = (raw - min_s) / rng\n","        rows.append({\"doc_id_1\": docids[i], \"doc_id_2\": docids[j],\n","                     \"bm25_raw\": raw, \"bm25_norm\": norm})\n","\n","pairs_df = pd.DataFrame(rows).sort_values(\"bm25_raw\", ascending=False)\n","pairs_df.to_csv(PAIRS_CSV, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","print(f\"[OK] Wrote BM25 pairs (tm) → {PAIRS_CSV}\")\n","\n","# 6) Quick peek\n","print(\"\\nTop 10 most similar pairs by BM25 (tm):\")\n","print(pairs_df.head(10).to_string(index=False))\n"],"metadata":{"id":"DSNlC4a9GwM2","executionInfo":{"status":"aborted","timestamp":1767987663285,"user_tz":300,"elapsed":387,"user":{"displayName":"ryan spencer","userId":"08004024603382603585"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["text_clean_tm_ngrams, builds BM25 similarities, and writes:\n","\n","bm25_matrix_tm_ngrams.csv\n","\n","bm25_pairs_tm_ngrams.csv"],"metadata":{"id":"5dMw8qA2HjeD"}},{"cell_type":"code","source":["# ============================================================\n","# BM25 (symmetric) pairwise similarities using text_clean_tm_ngrams\n","# Reads:  /content/drive/MyDrive/webscrape_links/master_links_preprocessed.csv\n","# Uses:   text_clean_tm_ngrams (space-joined tokens w/ bigrams+trigrams)\n","# Writes: bm25_matrix_tm_ngrams.csv (NxN) and bm25_pairs_tm_ngrams.csv (long form)\n","# ============================================================\n","\n","\n","from pathlib import Path\n","import csv\n","import numpy as np\n","import pandas as pd\n","\n","# 0) Paths\n","BASE_DIR   = Path(\"/content/drive/MyDrive/webscrape_links\")\n","IN_PATH    = BASE_DIR / \"master_links_preprocessed.csv\"\n","MATRIX_CSV = BASE_DIR / \"bm25_matrix_tm_ngrams.csv\"\n","PAIRS_CSV  = BASE_DIR / \"bm25_pairs_tm_ngrams.csv\"\n","\n","# 1) Load master\n","df = pd.read_csv(IN_PATH, dtype=str, keep_default_na=False, quoting=csv.QUOTE_ALL)\n","assert \"doc_id\" in df.columns, \"Missing 'doc_id' in master.\"\n","assert \"text_clean_tm_ngrams\" in df.columns, \"Missing 'text_clean_tm_ngrams' in master. Run preprocessing first.\"\n","\n","# Token lists from space-joined n-gram stream\n","docs_tokens = [(t or \"\").split() for t in df[\"text_clean_tm_ngrams\"].tolist()]\n","docids      = df[\"doc_id\"].fillna(\"\").astype(str).tolist()\n","N = len(docs_tokens)\n","if N == 0:\n","    raise RuntimeError(\"No documents found.\")\n","\n","# 2) BM25 (install rank_bm25 if needed)\n","try:\n","    from rank_bm25 import BM25Okapi\n","except Exception:\n","    !pip -q install rank-bm25==0.2.2\n","    from rank_bm25 import BM25Okapi\n","\n","# Build BM25 index on the corpus (n-gram tokens)\n","bm25 = BM25Okapi(docs_tokens)  # k1=1.5, b=0.75 defaults\n","\n","# Asymmetric matrix: score(query_i, doc_j)\n","asym = np.zeros((N, N), dtype=float)\n","for i, q in enumerate(docs_tokens):\n","    asym[i, :] = bm25.get_scores(q)\n","\n","# 3) Symmetric similarity: S(i,j) = (BM25(i→j) + BM25(j→i)) / 2\n","sym = (asym + asym.T) / 2.0\n","\n","# Diagonal: set to row max for readability (self-sim highest)\n","for i in range(N):\n","    sym[i, i] = float(sym[i, :].max()) if N else 0.0\n","\n","# 4) Save matrix CSV (NxN with doc_id headers)\n","matrix_df = pd.DataFrame(sym, index=docids, columns=docids)\n","matrix_df.to_csv(MATRIX_CSV, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","print(f\"[OK] Wrote BM25 symmetric matrix (tm_ngrams) → {MATRIX_CSV}\")\n","\n","# 5) Save pairs CSV (upper triangle, excluding diagonal), with normalized score\n","rows = []\n","off_diag = []\n","for i in range(N):\n","    for j in range(i+1, N):\n","        off_diag.append(sym[i, j])\n","\n","min_s = float(min(off_diag)) if off_diag else 0.0\n","max_s = float(max(off_diag)) if off_diag else 1.0\n","rng   = (max_s - min_s) if (max_s > min_s) else 1.0\n","\n","for i in range(N):\n","    for j in range(i+1, N):\n","        raw = float(sym[i, j])\n","        norm = (raw - min_s) / rng\n","        rows.append({\n","            \"doc_id_1\": docids[i],\n","            \"doc_id_2\": docids[j],\n","            \"bm25_raw\": raw,\n","            \"bm25_norm\": norm\n","        })\n","\n","pairs_df = pd.DataFrame(rows).sort_values(\"bm25_raw\", ascending=False)\n","pairs_df.to_csv(PAIRS_CSV, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","print(f\"[OK] Wrote BM25 pairs (tm_ngrams) → {PAIRS_CSV}\")\n","\n","# 6) Quick peek\n","print(\"\\nTop 10 most similar pairs by BM25 (tm_ngrams):\")\n","print(pairs_df.head(10).to_string(index=False))\n"],"metadata":{"id":"bxa0gfxyHkGK","executionInfo":{"status":"aborted","timestamp":1767987663294,"user_tz":300,"elapsed":395,"user":{"displayName":"ryan spencer","userId":"08004024603382603585"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["semantic pairwise similarity with all-mpnet-base-v2 (and gracefully falls back to all-MiniLM-L6-v2 if needed). It:\n","\n","reads /content/drive/MyDrive/webscrape_links/master_links_preprocessed.csv\n","\n","uses text_clean_llm (best for semantics; falls back to text_en / text)\n","\n","chunks long docs by token count (overlapped), embeds each chunk, then weighted-averages per doc\n","\n","normalizes embeddings and computes a cosine similarity matrix\n","\n","writes:\n","\n","semantic_sim_matrix_mpnet.csv (NxN, rows/cols = doc_id)\n","semantic_sim_pairs_mpnet.csv (upper-triangle pairs, sorted desc)"],"metadata":{"id":"DPNDvzp4IA-K"}},{"cell_type":"code","source":["# Clean out conflicting installs, then pin a compatible set and restart.\n","!pip -q uninstall -y transformers tokenizers sentence-transformers huggingface-hub accelerate peft bitsandbytes optimum\n","\n"],"metadata":{"id":"FNouXwEWINDd","executionInfo":{"status":"aborted","timestamp":1767987663301,"user_tz":300,"elapsed":185896,"user":{"displayName":"ryan spencer","userId":"08004024603382603585"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","!pip -q install --no-cache-dir \\\n","  \"transformers\" \\\n","  \"tokenizers\" \\\n","  \"huggingface-hub\" \\\n","  \"accelerate\" \\\n","  \"sentence-transformers\" \\\n","  \"scikit-learn\"\n","\n","import os\n","print(\"✅ HF stack aligned. Restarting runtime to load clean binaries…\")\n","os.kill(os.getpid(), 9)  # Colab-safe restart"],"metadata":{"id":"dG0--KSuL3J6","executionInfo":{"status":"aborted","timestamp":1767987663307,"user_tz":300,"elapsed":185901,"user":{"displayName":"ryan spencer","userId":"08004024603382603585"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# Semantic similarity (pairwise) with all-MiniLM-L6-v2\n","# Input:  /content/drive/MyDrive/webscrape_links/master_links_preprocessed.csv\n","# Text:   text_clean_llm (fallback -> text_en -> text)\n","# Output: semantic_sim_matrix_minilm.csv  (NxN cosine)\n","#         semantic_sim_pairs_minilm.csv   (tidy pairs)\n","# ============================================================\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","from pathlib import Path\n","import csv, re\n","import numpy as np\n","import pandas as pd\n","import torch\n","\n","from sentence_transformers import SentenceTransformer\n","from huggingface_hub import snapshot_download\n","\n","# ---- Paths ----\n","BASE_DIR   = Path(\"/content/drive/MyDrive/webscrape_links\")\n","IN_PATH    = BASE_DIR / \"master_links_preprocessed.csv\"\n","OUT_MAT    = BASE_DIR / \"semantic_sim_matrix_minilm.csv\"\n","OUT_PAIRS  = BASE_DIR / \"semantic_sim_pairs_minilm.csv\"\n","CACHE_DIR  = Path(\"/content/hf_models\"); CACHE_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# ---- Load data ----\n","df = pd.read_csv(IN_PATH, dtype=str, keep_default_na=False, quoting=csv.QUOTE_ALL)\n","assert \"doc_id\" in df.columns, \"Missing 'doc_id' in master.\"\n","if \"text_clean_llm\" in df.columns and df[\"text_clean_llm\"].str.strip().any():\n","    SRC_COL = \"text_clean_llm\"\n","elif \"text_en\" in df.columns and df[\"text_en\"].str.strip().any():\n","    SRC_COL = \"text_en\"\n","elif \"text\" in df.columns and df[\"text\"].str.strip().any():\n","    SRC_COL = \"text\"\n","else:\n","    raise RuntimeError(\"No usable text column found (need one of: text_clean_llm, text_en, text).\")\n","\n","texts  = df[SRC_COL].fillna(\"\").astype(str).tolist()\n","docids = df[\"doc_id\"].fillna(\"\").astype(str).tolist()\n","N = len(texts)\n","print(f\"[INFO] Using column for semantics: {SRC_COL} (docs={N})\")\n","\n","# ---- Load model: all-MiniLM-L6-v2 (direct → HF local fallback) ----\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_repo = \"sentence-transformers/all-MiniLM-L6-v2\"\n","\n","def load_minilm(repo_id: str):\n","    try:\n","        print(f\"[INFO] Loading model directly: {repo_id} on {device}\")\n","        return SentenceTransformer(repo_id, device=device), \"direct\"\n","    except Exception as e:\n","        print(f\"[WARN] Direct load failed: {e}\")\n","        print(f\"[INFO] Downloading HF repo → local path…\")\n","        local_dir = snapshot_download(\n","            repo_id=repo_id,\n","            local_dir=str(CACHE_DIR / repo_id.replace(\"/\", \"_\")),\n","            local_dir_use_symlinks=False\n","        )\n","        print(f\"[INFO] Loading from local path: {local_dir}\")\n","        return SentenceTransformer(local_dir, device=device), \"local\"\n","\n","model, load_mode = load_minilm(model_repo)\n","print(f\"[OK] Model ready ({load_mode}).\")\n","\n","tokenizer = model.tokenizer\n","max_model_tokens = getattr(model, \"max_seq_length\", 512) or 512\n","MAX_TOKENS = min(420, max_model_tokens - 32)   # content tokens per chunk\n","STRIDE     = max(40, MAX_TOKENS // 6)          # overlap tokens\n","BATCH_SIZE = 8 if device == \"cpu\" else 24\n","\n","# ---- Token-aware chunking ----\n","import re\n","def count_tokens(text: str) -> int:\n","    if not text: return 0\n","    return len(tokenizer.encode(text, add_special_tokens=False))\n","\n","def chunk_by_tokens(text: str, max_tokens: int = MAX_TOKENS, stride: int = STRIDE):\n","    text = (text or \"\").strip()\n","    if not text:\n","        return []\n","    paras = re.split(r\"\\n{2,}\", text)\n","    chunks, buf, buf_tok = [], \"\", 0\n","    def flush():\n","        nonlocal buf, buf_tok\n","        if buf:\n","            chunks.append((buf.strip(), buf_tok))\n","            buf, buf_tok = \"\", 0\n","    for p in paras:\n","        p = p.strip()\n","        if not p: continue\n","        ptoks = count_tokens(p)\n","        if ptoks <= max_tokens:\n","            if buf_tok + ptoks + 2 <= max_tokens:\n","                buf = buf + (\"\\n\\n\" if buf else \"\") + p\n","                buf_tok += ptoks + (2 if buf else 0)\n","            else:\n","                flush(); buf, buf_tok = p, ptoks\n","        else:\n","            sents = re.split(r\"(?<=[\\.\\!\\?\\:;])\\s+\", p)\n","            for s in sents:\n","                s = s.strip()\n","                if not s: continue\n","                stoks = count_tokens(s)\n","                if stoks > max_tokens:\n","                    words = s.split()\n","                    cur, cur_tok = [], 0\n","                    for w in words:\n","                        wt = count_tokens(w + \" \")\n","                        if cur_tok + wt > max_tokens:\n","                            chunks.append((\" \".join(cur), cur_tok))\n","                            if STRIDE > 0 and cur:\n","                                tail = \" \".join(cur[-max(1, len(cur)//4):])\n","                                chunks.append((tail, count_tokens(tail)))\n","                            cur, cur_tok = [w], wt\n","                        else:\n","                            cur.append(w); cur_tok += wt\n","                    if cur:\n","                        chunks.append((\" \".join(cur), cur_tok))\n","                else:\n","                    if buf_tok + stoks + 1 <= max_tokens:\n","                        buf = (buf + \" \" + s).strip(); buf_tok += stoks + 1\n","                    else:\n","                        flush(); buf, buf_tok = s, stoks\n","            flush()\n","    flush()\n","    # light overlap tails\n","    if STRIDE > 0 and len(chunks) > 1:\n","        with_overlap = []\n","        for i, (txt_i, tok_i) in enumerate(chunks):\n","            with_overlap.append((txt_i, tok_i))\n","            if i < len(chunks) - 1:\n","                tail_words = txt_i.split()[-min(100, len(txt_i.split())):]\n","                tail_text = \" \".join(tail_words)\n","                tail_tok = count_tokens(tail_text)\n","                if tail_tok > 0:\n","                    with_overlap.append((tail_text, tail_tok))\n","        chunks = with_overlap\n","    # dedupe small overlaps\n","    deduped, seen = [], set()\n","    for t, k in chunks:\n","        key = (t[:120], k)\n","        if key in seen:\n","            continue\n","        seen.add(key)\n","        deduped.append((t, k))\n","    return deduped\n","\n","# ---- Embed a document (weighted mean of chunk embeddings; L2-normalized) ----\n","def embed_document(text: str, batch_size: int = BATCH_SIZE):\n","    chs = chunk_by_tokens(text, MAX_TOKENS, STRIDE)\n","    if not chs:\n","        return None\n","    ctexts  = [t for t, _ in chs]\n","    weights = np.array([max(1, k) for _, k in chs], dtype=np.float32)\n","    embs = model.encode(\n","        ctexts,\n","        batch_size=batch_size,\n","        convert_to_numpy=True,\n","        normalize_embeddings=True,\n","        show_progress_bar=False,\n","    ).astype(np.float32)\n","    num = (embs * weights[:, None]).sum(axis=0)\n","    den = weights.sum()\n","    vec = num / max(1e-8, den)\n","    vec = vec / max(1e-8, np.linalg.norm(vec))\n","    return vec\n","\n","# ---- Embed all docs ----\n","doc_vecs, missing = [], 0\n","for i, txt in enumerate(texts, 1):\n","    if not txt.strip():\n","        doc_vecs.append(None); missing += 1\n","        print(f\"[WARN] Empty text for doc {docids[i-1]}\")\n","        continue\n","    doc_vecs.append(embed_document(txt))\n","    if i % 3 == 0 or i == N:\n","        print(f\"[INFO] Embedded {i}/{N}\")\n","\n","# Fill empties with zeros to keep shape\n","dim = next((v.shape[0] for v in doc_vecs if v is not None), None)\n","if dim is None:\n","    raise RuntimeError(\"All embeddings are empty.\")\n","for i, v in enumerate(doc_vecs):\n","    if v is None:\n","        doc_vecs[i] = np.zeros(dim, dtype=np.float32)\n","\n","E = np.vstack(doc_vecs).astype(np.float32)  # (N, D), row-normalized\n","\n","# Cosine similarity as dot product\n","sim = (E @ E.T).astype(np.float32)\n","sim = np.clip(sim, -1.0, 1.0)\n","np.fill_diagonal(sim, 1.0)\n","\n","# ---- Save outputs ----\n","matrix_df = pd.DataFrame(sim, index=docids, columns=docids)\n","matrix_df.to_csv(OUT_MAT, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","print(f\"[OK] Wrote semantic cosine matrix (all-MiniLM-L6-v2) → {OUT_MAT}\")\n","\n","rows = []\n","for i in range(N):\n","    for j in range(i+1, N):\n","        rows.append({\"doc_id_1\": docids[i], \"doc_id_2\": docids[j], \"cosine\": float(sim[i, j])})\n","pairs_df = pd.DataFrame(rows).sort_values(\"cosine\", ascending=False)\n","pairs_df.to_csv(OUT_PAIRS, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","print(f\"[OK] Wrote semantic cosine pairs (all-MiniLM-L6-v2) → {OUT_PAIRS}\")\n","\n","print(\"\\nTop 10 most similar pairs (semantic):\")\n","print(pairs_df.head(10).to_string(index=False))\n"],"metadata":{"id":"AiVTVOfvHm8G","executionInfo":{"status":"aborted","timestamp":1767987663310,"user_tz":300,"elapsed":185904,"user":{"displayName":"ryan spencer","userId":"08004024603382603585"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Heatmap plot for whichever similarity matrix exists.\n","# Priority: semantic → TF-IDF → BM25\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","\n","BASE = Path(\"/content/drive/MyDrive/webscrape_links\")\n","candidates = [\n","    (\"Semantic (MiniLM)\", BASE / \"semantic_sim_matrix_minilm.csv\"),\n","    (\"Semantic (MPNet)\",  BASE / \"semantic_sim_matrix_mpnet.csv\"),\n","    (\"TF-IDF (tm)\",       BASE / \"tfidf_cosine_matrix_tm.csv\"),\n","    (\"TF-IDF (ngrams)\",   BASE / \"tfidf_cosine_matrix.csv\"),\n","    (\"BM25 (tm)\",         BASE / \"bm25_matrix_tm.csv\"),\n","    (\"BM25 (tm_ngrams)\",  BASE / \"bm25_matrix_tm_ngrams.csv\"),\n","]\n","\n","chosen_title, df_mat = None, None\n","for title, path in candidates:\n","    if path.exists():\n","        df_mat = pd.read_csv(path, index_col=0)\n","        chosen_title = title\n","        break\n","\n","if df_mat is None:\n","    raise FileNotFoundError(\"No similarity matrices found among expected files. \"\n","                            \"Please generate one (semantic/TF-IDF/BM25) first.\")\n","\n","# Plot heatmap (matplotlib only, no custom colors)\n","plt.figure(figsize=(max(10, 0.6 * df_mat.shape[1]), max(8, 0.6 * df_mat.shape[0])))\n","im = plt.imshow(df_mat.values, aspect=\"auto\", interpolation=\"nearest\", vmin=0.0, vmax=1.0)\n","plt.xticks(range(df_mat.shape[1]), df_mat.columns, rotation=90)\n","plt.yticks(range(df_mat.shape[0]), df_mat.index)\n","plt.colorbar(im, fraction=0.046, pad=0.04, label=\"Similarity\")\n","plt.title(f\"Similarity Heatmap • {chosen_title}\")\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"hwSN2zV2IZGy","executionInfo":{"status":"aborted","timestamp":1767987663313,"user_tz":300,"elapsed":185906,"user":{"displayName":"ryan spencer","userId":"08004024603382603585"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================\n","# Make individual heatmaps for all available outputs:\n","#   - Semantic (MiniLM, MPNet)\n","#   - TF-IDF (tm, ngrams)\n","#   - BM25 (tm, tm_ngrams)\n","# Saves PNGs to: /content/drive/MyDrive/webscrape_links/viz/heatmaps/\n","# Also displays each heatmap inline (one figure per chart).\n","# ============================================================\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","from pathlib import Path\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","BASE = Path(\"/content/drive/MyDrive/webscrape_links\")\n","OUTDIR = BASE / \"viz\" / \"heatmaps\"\n","OUTDIR.mkdir(parents=True, exist_ok=True)\n","\n","# ---- Helper: reconstruct a symmetric matrix from pairs CSV ----\n","def matrix_from_pairs(pairs_path: Path, doc_id_cols=(\"doc_id_1\",\"doc_id_2\"), value_col=\"cosine\"):\n","    dfp = pd.read_csv(pairs_path, dtype=str)\n","    # ensure numeric\n","    dfp[value_col] = pd.to_numeric(dfp[value_col], errors=\"coerce\").fillna(0.0)\n","    doc_ids = sorted(set(dfp[doc_id_cols[0]]).union(set(dfp[doc_id_cols[1]])))\n","    idx = {d:i for i,d in enumerate(doc_ids)}\n","    N = len(doc_ids)\n","    M = np.eye(N, dtype=np.float32)\n","    for _, r in dfp.iterrows():\n","        i = idx[r[doc_id_cols[0]]]; j = idx[r[doc_id_cols[1]]]\n","        v = float(r[value_col])\n","        M[i, j] = v\n","        M[j, i] = v\n","    return pd.DataFrame(M, index=doc_ids, columns=doc_ids)\n","\n","# ---- Helper: normalize a matrix to [0,1] using off-diagonal min/max ----\n","def minmax_offdiag(df_mat: pd.DataFrame):\n","    arr = df_mat.values.astype(float).copy()\n","    n = arr.shape[0]\n","    off = [arr[i, j] for i in range(n) for j in range(n) if i != j]\n","    if not off:\n","        return df_mat\n","    mn, mx = float(min(off)), float(max(off))\n","    rng = (mx - mn) if mx > mn else 1.0\n","    for i in range(n):\n","        for j in range(n):\n","            if i != j:\n","                arr[i, j] = (arr[i, j] - mn) / rng\n","            else:\n","                arr[i, j] = 1.0\n","    return pd.DataFrame(arr, index=df_mat.index, columns=df_mat.columns)\n","\n","# ---- Spec for each output we’ll try to plot ----\n","# Each entry: (title, kind, matrix_csv, pairs_csv, pairs_value_col)\n","# kind: 'cosine' (fixed [0,1] scale) or 'bm25' (normalize)\n","specs = [\n","    (\"Semantic Similarity • MiniLM\", \"cosine\",\n","     BASE / \"semantic_sim_matrix_minilm.csv\",\n","     BASE / \"semantic_sim_pairs_minilm.csv\", \"cosine\"),\n","    (\"Semantic Similarity • MPNet\", \"cosine\",\n","     BASE / \"semantic_sim_matrix_mpnet.csv\",\n","     BASE / \"semantic_sim_pairs_mpnet.csv\", \"cosine\"),\n","    (\"TF-IDF Cosine • tm (unigrams)\", \"cosine\",\n","     BASE / \"tfidf_cosine_matrix_tm.csv\",\n","     BASE / \"tfidf_cosine_pairs_tm.csv\", \"cosine\"),\n","    (\"TF-IDF Cosine • n-grams\", \"cosine\",\n","     BASE / \"tfidf_cosine_matrix.csv\",\n","     BASE / \"tfidf_cosine_pairs.csv\", \"cosine\"),\n","    (\"BM25 • tm (normalized for plot)\", \"bm25\",\n","     BASE / \"bm25_matrix_tm.csv\",\n","     BASE / \"bm25_pairs_tm.csv\", \"bm25_norm\"),\n","    (\"BM25 • tm_n-grams (normalized for plot)\", \"bm25\",\n","     BASE / \"bm25_matrix_tm_ngrams.csv\",\n","     BASE / \"bm25_pairs_tm_ngrams.csv\", \"bm25_norm\"),\n","]\n","\n","made_any = False\n","for title, kind, mpath, ppath, pcol in specs:\n","    try:\n","        df_mat = None\n","        # Prefer matrix CSV if present\n","        if mpath.exists():\n","            df_mat = pd.read_csv(mpath, index_col=0)\n","            # For BM25 raw matrices, normalize for visual comparability\n","            if kind == \"bm25\":\n","                df_mat = minmax_offdiag(df_mat)\n","        # Else try reconstructing from pairs\n","        elif ppath.exists():\n","            df_mat = matrix_from_pairs(ppath, value_col=pcol)\n","            if kind == \"bm25\" and pcol != \"bm25_norm\":\n","                df_mat = minmax_offdiag(df_mat)\n","        else:\n","            # skip if neither exists\n","            continue\n","\n","        # Plot single-chart heatmap (no custom colors)\n","        plt.figure(figsize=(max(10, 0.6 * df_mat.shape[1]), max(8, 0.6 * df_mat.shape[0])))\n","        if kind == \"cosine\":\n","            im = plt.imshow(df_mat.values, aspect=\"auto\", interpolation=\"nearest\", vmin=0.0, vmax=1.0)\n","            cbar_label = \"Cosine similarity\"\n","        else:\n","            im = plt.imshow(df_mat.values, aspect=\"auto\", interpolation=\"nearest\", vmin=0.0, vmax=1.0)\n","            cbar_label = \"Similarity (normalized)\"\n","        plt.xticks(range(df_mat.shape[1]), df_mat.columns, rotation=90)\n","        plt.yticks(range(df_mat.shape[0]), df_mat.index)\n","        plt.colorbar(im, fraction=0.046, pad=0.04, label=cbar_label)\n","        plt.title(title)\n","        plt.tight_layout()\n","\n","        # Save PNG\n","        out_png = OUTDIR / (mpath.stem + \".png\" if mpath.exists() else ppath.stem + \".png\")\n","        plt.savefig(out_png, dpi=150)\n","        plt.show()   # one figure per chart\n","        print(f\"[OK] Saved heatmap → {out_png}\")\n","        made_any = True\n","    except Exception as e:\n","        print(f\"[WARN] Skipped '{title}': {e}\")\n","\n","if not made_any:\n","    print(\"No expected similarity outputs were found. Generate matrices/pairs first (semantic / TF-IDF / BM25).\")\n"],"metadata":{"id":"lpG183c5OI0z","executionInfo":{"status":"aborted","timestamp":1767987663318,"user_tz":300,"elapsed":185910,"user":{"displayName":"ryan spencer","userId":"08004024603382603585"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LDA topic modeling across your corpus (using your preprocessed tokens), then gives you per-document topics and topic summaries. It also saves a doc–topic heatmap.\n","\n","Input: /content/drive/MyDrive/webscrape_links/master_links_preprocessed.csv\n","\n","Text used: prefers text_clean_tm_ngrams (phrases), falls back to text_clean_tm\n","\n","Auto-selects number of topics K by trying a small range and picking the best perplexity\n","\n","Outputs (all in /content/drive/MyDrive/webscrape_links/topics/):\n","\n","lda_topics_summary.csv — one row per topic with top terms\n","\n","lda_doc_topics.csv — one row per document with dominant topic + top3\n","\n","lda_doc_topic_matrix.csv — full doc×topic probabilities\n","\n","viz/lda_doc_topic_heatmap.png — heatmap of doc×topic (also shown inline)\n","\n","# ============================================================\n","# LDA Topic Modeling (per-document assignments + topic summaries)\n","# Reads:  /content/drive/MyDrive/webscrape_links/master_links_preprocessed.csv\n","# Uses:   text_clean_tm_ngrams (preferred) or text_clean_tm\n","# Writes: /content/drive/MyDrive/webscrape_links/topics/\n","#         - lda_topics_summary.csv\n","#         - lda_doc_topics.csv\n","#         - lda_doc_topic_matrix.csv\n","#         - viz/lda_doc_topic_heatmap.png (shown inline)\n","# ============================================================\n"],"metadata":{"id":"itUkUZfBPsjO"}},{"cell_type":"code","source":["# ============================================================\n","# LDA Topic Modeling (per-document assignments + topic summaries)\n","# Reads:  /content/drive/MyDrive/webscrape_links/master_links_preprocessed.csv\n","# Uses:   text_clean_tm_ngrams (preferred) or text_clean_tm\n","# Writes: /content/drive/MyDrive/webscrape_links/topics/\n","#         - lda_topics_summary.csv\n","#         - lda_doc_topics.csv\n","#         - lda_doc_topic_matrix.csv\n","#         - viz/lda_doc_topic_heatmap.png (shown inline)\n","# ============================================================\n","\n","\n","from pathlib import Path\n","import csv, math, numpy as np, pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# sklearn\n","try:\n","    from sklearn.feature_extraction.text import CountVectorizer\n","    from sklearn.decomposition import LatentDirichletAllocation\n","except Exception:\n","    !pip -q install scikit-learn==1.5.1\n","    from sklearn.feature_extraction.text import CountVectorizer\n","    from sklearn.decomposition import LatentDirichletAllocation\n","\n","# -------------------------\n","# Paths\n","# -------------------------\n","BASE = Path(\"/content/drive/MyDrive/webscrape_links\")\n","IN_PATH = BASE / \"master_links_preprocessed.csv\"\n","OUT_DIR = BASE / \"topics\"\n","VIZ_DIR = OUT_DIR / \"viz\"\n","OUT_DIR.mkdir(parents=True, exist_ok=True)\n","VIZ_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# -------------------------\n","# Load data\n","# -------------------------\n","df = pd.read_csv(IN_PATH, dtype=str, keep_default_na=False, quoting=csv.QUOTE_ALL)\n","assert \"doc_id\" in df.columns, \"Missing 'doc_id' in master.\"\n","\n","if \"text_clean_tm_ngrams\" in df.columns and df[\"text_clean_tm_ngrams\"].str.strip().any():\n","    SRC_COL = \"text_clean_tm_ngrams\"\n","    token_pattern = r\"(?u)\\b\\w[\\w_]+\\b\"  # keep underscores for phrases\n","else:\n","    assert \"text_clean_tm\" in df.columns, \"Missing 'text_clean_tm' and 'text_clean_tm_ngrams'.\"\n","    SRC_COL = \"text_clean_tm\"\n","    token_pattern = r\"(?u)\\b\\w+\\b\"\n","\n","texts = df[SRC_COL].fillna(\"\").astype(str).tolist()\n","docids = df[\"doc_id\"].fillna(\"\").astype(str).tolist()\n","N = len(texts)\n","print(f\"[INFO] Source column: {SRC_COL} | docs={N}\")\n","\n","# -------------------------\n","# Vectorize (bag of words)\n","# -------------------------\n","# Documents are already cleaned; no lowercase/stopwords needed here.\n","vectorizer = CountVectorizer(\n","    lowercase=False,\n","    token_pattern=token_pattern,\n","    min_df=1,\n","    max_df=0.95,\n",")\n","X = vectorizer.fit_transform(texts)   # shape: (N_docs, Vocab)\n","vocab = np.array(vectorizer.get_feature_names_out())\n","print(f\"[INFO] Vocab size: {len(vocab)}\")\n","\n","# Guard: if vocab is empty, bail gracefully\n","if X.shape[1] == 0:\n","    raise RuntimeError(\"Vectorizer produced an empty vocabulary. Check your preprocessed text columns.\")\n","\n","# -------------------------\n","# Choose K via quick perplexity search\n","# -------------------------\n","def choose_k(X, k_candidates):\n","    best_k, best_pp = None, float(\"inf\")\n","    for k in k_candidates:\n","        lda = LatentDirichletAllocation(\n","            n_components=k,\n","            learning_method=\"batch\",\n","            max_iter=20,\n","            random_state=42,\n","            evaluate_every=-1,\n","            doc_topic_prior=None,    # default 1/k\n","            topic_word_prior=None,   # default 1/k\n","        )\n","        lda.fit(X)\n","        # Note: sklearn's perplexity: lower is better\n","        pp = lda.perplexity(X)\n","        print(f\"  K={k:<2} perplexity={pp:.2f}\")\n","        if pp < best_pp:\n","            best_k, best_pp = k, pp\n","    return best_k, best_pp\n","\n","if N <= 6:\n","    k_cands = list(range(2, max(3, N)))     # tiny corpora\n","else:\n","    k_cands = list(range(3, min(10, N) + 1))\n","\n","print(\"[INFO] Selecting K (topics) via perplexity…\")\n","best_k, best_pp = choose_k(X, k_cands)\n","print(f\"[INFO] Selected K={best_k} (perplexity={best_pp:.2f})\")\n","\n","# -------------------------\n","# Final LDA fit (more iterations)\n","# -------------------------\n","lda = LatentDirichletAllocation(\n","    n_components=best_k,\n","    learning_method=\"batch\",\n","    max_iter=100,\n","    random_state=42,\n","    evaluate_every=-1,\n",")\n","lda.fit(X)\n","\n","# Topic-word distributions (beta), Doc-topic distributions (theta)\n","topic_word = lda.components_                  # shape: (K, V)\n","# Normalize rows to probabilities\n","topic_word = topic_word / topic_word.sum(axis=1, keepdims=True)\n","doc_topic = lda.transform(X)                  # shape: (N, K)\n","# Handle any NaNs/zeros (e.g., empty docs)\n","doc_topic = np.nan_to_num(doc_topic, nan=0.0, posinf=0.0, neginf=0.0)\n","row_sums = doc_topic.sum(axis=1, keepdims=True)\n","row_sums[row_sums == 0] = 1.0\n","doc_topic = doc_topic / row_sums\n","\n","# -------------------------\n","# Summaries\n","# -------------------------\n","def top_terms_for_topic(topic_idx, topn=15):\n","    weights = topic_word[topic_idx]\n","    top_idx = np.argsort(weights)[::-1][:topn]\n","    terms = vocab[top_idx]\n","    probs = weights[top_idx]\n","    return list(zip(terms, probs))\n","\n","# Topic summary table\n","rows = []\n","for t in range(best_k):\n","    pairs = top_terms_for_topic(t, topn=15)\n","    top_terms = \", \".join([w for w, _ in pairs])\n","    rows.append({\n","        \"topic_id\": t,\n","        \"top_terms\": top_terms,\n","        \"top_terms_with_weights\": \"; \".join([f\"{w}:{p:.4f}\" for w, p in pairs])\n","    })\n","topics_summary = pd.DataFrame(rows)\n","\n","# Per-document assignments\n","doc_rows = []\n","for i, doc_id in enumerate(docids):\n","    dist = doc_topic[i]\n","    top_order = np.argsort(dist)[::-1]\n","    dom = int(top_order[0])\n","    dom_prob = float(dist[dom])\n","    top3 = [(int(t), float(dist[t])) for t in top_order[:3]]\n","    doc_rows.append({\n","        \"doc_id\": doc_id,\n","        \"dominant_topic\": dom,\n","        \"dominant_prob\": f\"{dom_prob:.4f}\",\n","        \"top3_topics\": \"; \".join([f\"{t}:{p:.4f}\" for t, p in top3])\n","    })\n","doc_topics = pd.DataFrame(doc_rows)\n","\n","# Full doc-topic matrix with labels\n","col_names = [f\"topic_{t}\" for t in range(best_k)]\n","doc_topic_df = pd.DataFrame(doc_topic, columns=col_names, index=docids).reset_index().rename(columns={\"index\":\"doc_id\"})\n","\n","# -------------------------\n","# Save outputs\n","# -------------------------\n","topics_summary.to_csv(OUT_DIR / \"lda_topics_summary.csv\", index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","doc_topics.to_csv(OUT_DIR / \"lda_doc_topics.csv\", index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","doc_topic_df.to_csv(OUT_DIR / \"lda_doc_topic_matrix.csv\", index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","\n","print(f\"[OK] Saved topics → {OUT_DIR / 'lda_topics_summary.csv'}\")\n","print(f\"[OK] Saved per-doc topics → {OUT_DIR / 'lda_doc_topics.csv'}\")\n","print(f\"[OK] Saved doc-topic matrix → {OUT_DIR / 'lda_doc_topic_matrix.csv'}\")\n","\n","# -------------------------\n","# Heatmap (doc × topic)\n","# -------------------------\n","plt.figure(figsize=(max(10, 0.6 * best_k), max(8, 0.6 * N)))\n","im = plt.imshow(doc_topic, aspect=\"auto\", interpolation=\"nearest\", vmin=0.0, vmax=1.0)\n","plt.xticks(range(best_k), [f\"T{t}\" for t in range(best_k)])\n","plt.yticks(range(N), docids)\n","plt.colorbar(im, fraction=0.046, pad=0.04, label=\"Topic probability\")\n","plt.title(f\"LDA Doc–Topic Heatmap (K={best_k})\")\n","plt.tight_layout()\n","out_png = VIZ_DIR / \"lda_doc_topic_heatmap.png\"\n","plt.savefig(out_png, dpi=150)\n","plt.show()\n","\n","print(f\"[OK] Heatmap saved → {out_png}\")\n"],"metadata":{"id":"bwfM807AOcOr","executionInfo":{"status":"aborted","timestamp":1767987663322,"user_tz":300,"elapsed":185913,"user":{"displayName":"ryan spencer","userId":"08004024603382603585"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["per-topic & per-document outputs and produces two matplotlib heatmaps:\n","\n","Doc × Topic probabilities (lda-like)\n","\n","Topic × Topic cosine similarity (via c-TF-IDF)"],"metadata":{"id":"w5y25-TDQ64i"}},{"cell_type":"code","source":["# ============================================================\n","# BERTopic (robust, no Transformers): TF-IDF -> SVD embeddings\n","# Corpus: /content/drive/MyDrive/webscrape_links/master_links_preprocessed.csv\n","# Text:   prefers text_clean_tm_ngrams, else text_clean_tm\n","# Outputs:\n","#   CSVs → /content/drive/MyDrive/webscrape_links/topics/\n","#     - bertopic_topic_info.csv\n","#     - bertopic_doc_topics.csv\n","#     - bertopic_doc_topic_matrix.csv\n","#   Heatmaps (PNG) → /content/drive/MyDrive/webscrape_links/topics/viz/\n","#     - bertopic_doc_topic_heatmap.png\n","#     - bertopic_topic_similarity_heatmap.png\n","# ============================================================\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Clean, minimal deps (no transformers/sentence-transformers headaches)\n","!pip -q install -U \"bertopic==0.16.3\" \"umap-learn==0.5.6\" \"hdbscan==0.8.33\" \"scikit-learn==1.5.1\"\n","\n","from pathlib import Path\n","import csv, numpy as np, pandas as pd, matplotlib.pyplot as plt\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.preprocessing import normalize\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","import umap\n","import hdbscan\n","from bertopic import BERTopic\n","\n","# -------------------------\n","# Paths\n","# -------------------------\n","BASE    = Path(\"/content/drive/MyDrive/webscrape_links\")\n","IN_PATH = BASE / \"master_links_preprocessed.csv\"\n","OUT_DIR = BASE / \"topics\"\n","VIZ_DIR = OUT_DIR / \"viz\"\n","OUT_DIR.mkdir(parents=True, exist_ok=True)\n","VIZ_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# -------------------------\n","# Load data\n","# -------------------------\n","df = pd.read_csv(IN_PATH, dtype=str, keep_default_na=False, quoting=csv.QUOTE_ALL)\n","assert \"doc_id\" in df.columns, \"Missing 'doc_id' in master.\"\n","\n","if \"text_clean_tm_ngrams\" in df.columns and df[\"text_clean_tm_ngrams\"].str.strip().any():\n","    SRC_COL = \"text_clean_tm_ngrams\"\n","    token_pattern = r\"(?u)\\b\\w[\\w_]+\\b\"  # keep n-grams with underscores\n","else:\n","    assert \"text_clean_tm\" in df.columns, \"Need 'text_clean_tm_ngrams' or 'text_clean_tm'.\"\n","    SRC_COL = \"text_clean_tm\"\n","    token_pattern = r\"(?u)\\b\\w+\\b\"\n","\n","docs   = df[SRC_COL].fillna(\"\").astype(str).tolist()\n","docids = df[\"doc_id\"].fillna(\"\").astype(str).tolist()\n","N = len(docs)\n","print(f\"[INFO] Using column: {SRC_COL} | docs={N}\")\n","\n","# -------------------------\n","# TF-IDF -> SVD Embeddings (dense, low-dim)\n","# -------------------------\n","vectorizer = TfidfVectorizer(\n","    lowercase=False,\n","    token_pattern=token_pattern,\n","    min_df=1,\n","    max_df=0.95,\n",")\n","X = vectorizer.fit_transform(docs)   # (N_docs, V)\n","if X.shape[1] == 0:\n","    raise RuntimeError(\"Empty vocabulary from TF-IDF. Check preprocessing.\")\n","\n","svd_dim = int(min(300, max(2, min(X.shape[0] - 1, X.shape[1] - 1))))\n","svd = TruncatedSVD(n_components=svd_dim, random_state=42)\n","emb = svd.fit_transform(X)           # (N_docs, svd_dim)\n","emb = normalize(emb)                 # L2 normalize\n","print(f\"[INFO] Built SVD embeddings: shape={emb.shape}\")\n","\n","# -------------------------\n","# BERTopic with explicit UMAP/HDBSCAN, no topic reduction (avoids KeyError)\n","# -------------------------\n","umap_model = umap.UMAP(\n","    n_neighbors=min(15, max(2, N-1)),\n","    n_components=min(10, svd_dim),\n","    metric=\"cosine\",\n","    random_state=42,\n","    low_memory=True,\n","    verbose=False,\n",")\n","hdbscan_model = hdbscan.HDBSCAN(\n","    min_cluster_size=2,\n","    min_samples=1,\n","    prediction_data=True,   # needed for probabilities\n","    gen_min_span_tree=False\n",")\n","\n","vectorizer_model = TfidfVectorizer(\n","    lowercase=False,\n","    token_pattern=token_pattern,\n","    min_df=1,\n","    max_df=0.95,\n",")\n","\n","def run_bertopic(calc_probs: bool = True):\n","    model = BERTopic(\n","        language=\"english\",\n","        umap_model=umap_model,\n","        hdbscan_model=hdbscan_model,\n","        vectorizer_model=vectorizer_model,\n","        min_topic_size=2,\n","        nr_topics=None,                # <-- disable 'auto' reduction to avoid KeyError\n","        calculate_probabilities=calc_probs,\n","        verbose=True,\n","    )\n","    topics, probs = model.fit_transform(docs, embeddings=emb)\n","    return model, topics, probs\n","\n","try:\n","    topic_model, topics, probs = run_bertopic(calc_probs=True)\n","except KeyError as e:\n","    print(f\"[WARN] Probabilities path raised {e}. Retrying without probabilities...\")\n","    topic_model, topics, probs = run_bertopic(calc_probs=False)\n","    probs = None\n","\n","topic_info = topic_model.get_topic_info()\n","topic_info.to_csv(OUT_DIR / \"bertopic_topic_info.csv\", index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","print(f\"[OK] Saved topic info → {OUT_DIR / 'bertopic_topic_info.csv'}\")\n","\n","# -------------------------\n","# Per-document topic assignments\n","# -------------------------\n","rows = []\n","for i, doc_id in enumerate(docids):\n","    t = int(topics[i])  # -1 = outlier\n","    p = \"\"\n","    if probs is not None and t >= 0 and t < probs.shape[1]:\n","        p = f\"{float(probs[i, t]):.4f}\"\n","    rows.append({\"doc_id\": doc_id, \"topic\": t, \"topic_probability\": p})\n","doc_topics = pd.DataFrame(rows)\n","doc_topics.to_csv(OUT_DIR / \"bertopic_doc_topics.csv\", index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","print(f\"[OK] Saved doc topics → {OUT_DIR / 'bertopic_doc_topics.csv'}\")\n","\n","# -------------------------\n","# Doc × Topic probability matrix (+ fallback if probs=None)\n","# -------------------------\n","if probs is not None:\n","    n_topics = probs.shape[1]\n","    col_names = [f\"T{t}\" for t in range(n_topics)]\n","    doc_topic_df = pd.DataFrame(probs, columns=col_names, index=docids).reset_index().rename(columns={\"index\":\"doc_id\"})\n","else:\n","    # Fallback: one-hot memberships (1 at assigned topic, 0 else), include -1 column if present\n","    valid = sorted(set(topics))\n","    topic_to_col = {t: f\"T{t}\" for t in valid}\n","    mat = np.zeros((N, len(valid)), dtype=float)\n","    for i, t in enumerate(topics):\n","        j = valid.index(t)\n","        mat[i, j] = 1.0\n","    doc_topic_df = pd.DataFrame(mat, columns=[topic_to_col[t] for t in valid], index=docids).reset_index().rename(columns={\"index\":\"doc_id\"})\n","    n_topics = len(valid)\n","doc_topic_df.to_csv(OUT_DIR / \"bertopic_doc_topic_matrix.csv\", index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL, lineterminator=\"\\n\")\n","print(f\"[OK] Saved doc-topic matrix → {OUT_DIR / 'bertopic_doc_topic_matrix.csv'}\")\n","\n","# -------------------------\n","# Heatmap 1: Doc × Topic\n","# -------------------------\n","M = doc_topic_df.drop(columns=[\"doc_id\"]).values\n","plt.figure(figsize=(max(10, 0.6 * n_topics), max(8, 0.6 * N)))\n","im = plt.imshow(M, aspect=\"auto\", interpolation=\"nearest\", vmin=0.0, vmax=1.0)\n","plt.xticks(range(n_topics), doc_topic_df.columns[1:], rotation=0)\n","plt.yticks(range(N), docids)\n","plt.colorbar(im, fraction=0.046, pad=0.04, label=(\"Topic probability\" if probs is not None else \"Membership (0/1)\"))\n","plt.title(\"BERTopic • Doc–Topic Heatmap\")\n","plt.tight_layout()\n","out_png = VIZ_DIR / \"bertopic_doc_topic_heatmap.png\"\n","plt.savefig(out_png, dpi=150)\n","plt.show()\n","print(f\"[OK] Heatmap saved → {out_png}\")\n","\n","# -------------------------\n","# Heatmap 2: Topic × Topic similarity (c-TF-IDF cosine)\n","# -------------------------\n","valid_topics = [tid for tid in topic_info[\"Topic\"].tolist() if tid != -1]\n","if len(valid_topics) >= 2 and hasattr(topic_model, \"c_tf_idf_\"):\n","    ctf = topic_model.c_tf_idf_\n","    try:\n","        ctf = ctf.toarray()\n","    except Exception:\n","        ctf = np.array(ctf)\n","\n","    # Align rows of cTF-IDF to valid topic ids\n","    matrix_order = sorted([k for k in topic_model.get_topics().keys() if k != -1])\n","    topic_to_row = {tid: i for i, tid in enumerate(matrix_order)}\n","    rows_idx = [topic_to_row[t] for t in valid_topics if t in topic_to_row]\n","    if rows_idx:\n","        ctf_sel = ctf[rows_idx, :]\n","        sim_tt = cosine_similarity(ctf_sel)\n","        plt.figure(figsize=(max(8, 0.6 * len(valid_topics)), max(8, 0.6 * len(valid_topics))))\n","        im2 = plt.imshow(sim_tt, aspect=\"auto\", interpolation=\"nearest\", vmin=0.0, vmax=1.0)\n","        xt = [f\"T{t}\" for t in valid_topics]\n","        plt.xticks(range(len(valid_topics)), xt, rotation=0)\n","        plt.yticks(range(len(valid_topics)), xt)\n","        plt.colorbar(im2, fraction=0.046, pad=0.04, label=\"Cosine similarity\")\n","        plt.title(\"BERTopic • Topic–Topic Similarity (c-TF-IDF)\")\n","        plt.tight_layout()\n","        out_png2 = VIZ_DIR / \"bertopic_topic_similarity_heatmap.png\"\n","        plt.savefig(out_png2, dpi=150)\n","        plt.show()\n","        print(f\"[OK] Heatmap saved → {out_png2}\")\n","    else:\n","        print(\"[WARN] Could not align topics to c-TF-IDF rows; skipping topic-topic heatmap.\")\n","else:\n","    print(\"[WARN] Not enough valid topics or missing c_tf_idf_; skipping topic-topic heatmap.\")\n","\n","# -------------------------\n","# Quick peek at topics\n","# -------------------------\n","print(\"\\nTop terms per topic:\")\n","for tid in valid_topics:\n","    terms = topic_model.get_topic(tid) or []\n","    top_terms = \", \".join([w for w, _ in terms[:10]])\n","    print(f\" T{tid}: {top_terms}\")\n"],"metadata":{"id":"9qQ705wKP3Lw","executionInfo":{"status":"aborted","timestamp":1767987663325,"user_tz":300,"elapsed":185915,"user":{"displayName":"ryan spencer","userId":"08004024603382603585"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fMiIgUKvSHrD","executionInfo":{"status":"aborted","timestamp":1767987663328,"user_tz":300,"elapsed":185917,"user":{"displayName":"ryan spencer","userId":"08004024603382603585"}}},"execution_count":null,"outputs":[]}]}